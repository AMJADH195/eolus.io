#!/bin/bash

# Another script in the GRIBLoader family.
# This one retrieves the GFS hourly 0.25 deg data.
#
# Thomas Horner | thomas@14ersForecast.net


# Looks through the remote directory of GRIB files
# to see what the last completed model run was.
function check_if_data_exists {
	FILE="http://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.$1$2/gfs.t$2z.pgrb2.0p25.f384"
	echo "Endpoint: $FILE"

	if curl --output /dev/null --silent --head --fail "$FILE"; then
		echo "------ It exists. ------"
		update_data $1 $2
	else
		echo "It doesn't exist."
	fi
}

# Sets the correct filename to be downloaded, then retrieves the
# GRIB subet from NCEP NOMADS, converts and reprojects it to TIF.
function download_and_convert {
	# $1 = model var (TMP, etc.); $2 = model level; $3 = date; $4 = filename; $5 = forecast hour; $6 = label; $7 = output filename; $8 = output directory
	echo ""
	echo "-----------------------------"
	echo "Downloading and converting..."
	echo "Parameter: $6 || Hour: $5"
	
	# Set some filenames
	URL="http://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_0p25_1hr.pl?file="
	URL+=$4
	URL+="&"
	URL+=$2
	URL+="&"
	URL+=$1
	URL+="&subregion=&leftlon=250&rightlon=258&toplat=42&bottomlat=36&dir=%2Fgfs."
	URL+=$3$9

	SHORTNAME=$8
	SHORTNAME+=$7
	SHORTNAME+=$5

	OUTPUTFILE=$SHORTNAME
	OUTPUTFILE+=".grib2"

	TIFF=$8
	TIFF+=$7
	TIFF+="_"
	TIFF+=`date -d "$3 $9 +$5 hour" +%Y%m%dT%H` 
	TIFF+="0000000Z.tif"
	
	# Download the GRIB subset from NCEP NOMADS
	echo "Downloading as $OUTPUTFILE."
	wget $URL -O $OUTPUTFILE -nv
	
	# GDAL helpfully reprojects and converts the filetype to TIF
	# We're going to use the EPSG:4326 projection
	echo "Reprojecting and converting, saving as $TIFF."
	gdalwarp $OUTPUTFILE $TIFF -t_srs EPSG:4326 -overwrite -multi -wo SOURCE_EXTRA=1000 --config CENTER_LONG 0 -co TILED=YES -co COMPRESS=NONE

	# Delete the GRIB
	rm $OUTPUTFILE
	echo "... Done."
	echo "-----------------------------"
	echo ""
}

# Loops through the 0-120 hour GFS forecast and grabs the file
# for each specified parameter type (temp, wind, precip, etc.)
function download_files {
	echo "Downloading files."
	for i in `seq -f %03g 000 240`;
        do
		#gfs.t18z.pgrb2.0p25.
		FILENAME="gfs.t"
		FILENAME+=$1
		FILENAME+="z.pgrb2.0p25.f"
		FILENAME+=$i
		download_and_convert "var_TMP=on" "lev_surface=on" $2 $FILENAME $i "Surface Temp" "SurfaceTemp" "/wxdata/gfs/tmp/" $1
		download_and_convert "var_SNOD=on" "lev_surface=on" $2 $FILENAME $i "Snow Depth" "SnowDepth" "/wxdata/gfs/snod/" $1
		download_and_convert "var_PRATE=on" "lev_surface=on" $2 $FILENAME $i "Precipitation Rate" "PrecipRate" "/wxdata/gfs/prate/" $1
		download_and_convert "var_APCP=on" "lev_surface=on" $2 $FILENAME $i "Precip Accum" "PrecipAccum" "/wxdata/gfs/apcp/" $1
		#download_and_convert "var_LTNG=on" "lev_surface=on" $2 $FILENAME $i "Lightning" "Lightning" "/wxdata/gfs/ltng/" $1
		download_and_convert "var_CAPE=on" "lev_surface=on" $2 $FILENAME $i "CAPE" "CAPE" "/wxdata/gfs/cape/" $1
		download_and_convert "var_CIN=on" "lev_surface=on" $2 $FILENAME $i "CIN" "CIN" "/wxdata/gfs/cin/" $1
		download_and_convert "var_GUST=on" "lev_surface=on" $2 $FILENAME $i "Gust Speed" "GustSpeed" "/wxdata/gfs/gust/" $1
		download_and_convert "var_TCDC=on" "lev_entire_atmosphere=on" $2 $FILENAME $i "Cloud Cover" "CloudCover" "/wxdata/gfs/tcdc/" $1
		download_and_convert "var_4LFTX=on" "lev_surface=on" $2 $FILENAME $i "Lifted Index" "LiftedIndex" "/wxdata/gfs/4lftx/" $1
		download_and_convert "var_UGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind U Component" "SurfaceWindU" "/wxdata/gfs/ugrd/" $1
		download_and_convert "var_VGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind V Component" "SurfaceWindV" "/wxdata/gfs/vgrd/" $1
		download_and_convert "var_CSNOW=on" "lev_surface=on" $2 $FILENAME $i "Categorical Snow" "CatSnow" "/wxdata/gfs/csnow/" $1
		download_and_convert "var_CICEP=on" "lev_surface=on" $2 $FILENAME $i "Categorical Ice Pellets" "CatIcePellets" "/wxdata/gfs/cicep/" $1
		download_and_convert "var_CFRZR=on" "lev_surface=on" $2 $FILENAME $i "Categorical Freezing Rain" "CatFrzRain" "/wxdata/gfs/cfrzr/" $1
		download_and_convert "var_CRAIN=on" "lev_surface=on" $2 $FILENAME $i "Categorical Rain" "CatRain" "/wxdata/gfs/crain/" $1
	done
}

# Checks to see if there's new data that needs to be downloaded.
# Keeps track of the last date of the model run with a local file
# 'gfsLastUpdated.txt' which just stores the date and time of the model run.
# If the script runs successfully it overwrites this date and time with the latest.
function update_data {
	echo "---------------------"
	echo "Update Data function."
	echo "CHECKDATE: $1"
	echo "CHECKTIME: $2"
	# check if we've already updated this
	read -r lastupdated<"gfsLastUpdated.txt"
	echo "Last updated: $lastupdated"	

	if [ $lastupdated = $1$2 ]; then
		echo "Nevermind, we've got this data already."
		exit 0
	fi
	
	echo "We need to get the new data."
	download_files $2 $1

	echo "Done, now cleaning up."
	echo "Deleting old files..."

	find /wxdata/ -type f -mtime +1 -name '*.tif' -execdir rm -- '{}' \;

	# Exit script
	echo "$1$2" > gfsLastUpdated.txt
	echo "You're all clear kid, now let's blow this thing and go home."
	exit 0
}	

##### MAIN #####
#set -e

echo "------------------"
echo "WX4Web GRIB Data Loader (GFS)"
echo "------------------"
DATE=`date +%Y%m%d`
YEAR=`date +%Y`
MONTH=`date +%m`
DAY=`date +%d`
echo "Initialized date variables."
echo "YEAR: $YEAR"
echo "MONTH: $MONTH"
echo "DAY: $DAY"
echo "------------"

# I can easily imagine a much better way of doing the below
# repetitive tasks...  but whatever.  Since there is considerable
# lag time publishing the models it looks back in time quite a ways.
# Not elegant, but it works.
#
# I didn't really think too much about what time the server actually
# thinks it is.  At least on my test server it uses UTC time
# so the month/day matches that of the model runs.  However
# if your date function is returning local time for some reason
# you may need to look ahead a day to capture the actual UTC date.

CHECKDATE=`date +%Y%m%d`
CHECKTIME="18"
echo "Checking 18Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="12"
echo "Checking 12Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="06"
echo "Checking 06Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="00"
echo "Checking 00Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="18"
echo "Checking 18Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="12"
echo "Checking 12Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="06"
echo "Checking 06Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="00"
echo "Checking 00Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME
