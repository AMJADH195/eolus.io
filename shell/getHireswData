#!/bin/bash

# getData - v1.0
#
# Automated script for retrieving weather models and adding
# them to GeoServer.  You'll want to run this as a cron job (every 15 minutes or so)
# Writes files to /wxdata/*.  Make sure permissions and chown is set properly.
# Writes to "latestversion.txt" - Make sure permissions and chown is set properly.
#
# Thomas Horner | thomas@14ersForecast.net
#
# Currently only loads NAM models.
#
# This is my first foray into actual Linux shell scripting.


# Looks through the remote directory of GRIB files
# to see what the last completed model run was.
# Not elegant, but functional enough.
function check_if_data_exists {
	FILE="http://nomads.ncep.noaa.gov/pub/data/nccf/com/hiresw/prod/hiresw.$1/hiresw.t$2.arw_5km.f48.conus.grib2"
	echo "Endpoint: $FILE"

	if curl --output /dev/null --silent --head --fail "$FILE"; then
		echo "------ It exists. ------"
		update_data $1 $2
	else
		echo "It doesn't exist."
	fi
}

# Sets the correct filename to be downloaded, then retrieves the
# GRIB subet from NCEP NOMADS, converts and reprojects it to TIF,
# and adds it to the GeoServer coverage store (layer is automatically created/updated)
function download_and_convert {
	# $1 = model var (TMP, etc.); $2 = model level; $3 = date; $4 = filename; $5 = forecast hour; $6 = label; $7 = output filename; $8 = output directory
	echo ""
	echo "-----------------------------"
	echo "Downloading and converting..."
	echo "Parameter: $6 || Hour: $5"
	
	# Set some filenames
	URL="http://nomads.ncep.noaa.gov/cgi-bin/filter_hiresconus.pl?file="
	URL+=$4
	URL+="&"
	URL+=$2
	URL+="&"
	URL+=$1
	URL+="&subregion=&leftlon=250&rightlon=258&toplat=42&bottomlat=36&dir=%2Fhiresw."
	URL+=$3

	SHORTNAME=$8
	SHORTNAME+=$7
	SHORTNAME+=$5

	OUTPUTFILE=$SHORTNAME
	OUTPUTFILE+=".grib2"

	TIFF=$8
	TIFF+=$7
	TIFF+="_"
	TIFF+=`date -d "$3 $9 +$5 hour" +%Y%m%dT%H` 
	TIFF+="0000000Z.tif"
	
	# Download the GRIB subset from NCEP NOMADS
	echo "Downloading as $OUTPUTFILE."
	wget $URL -O $OUTPUTFILE -nv
	
	# GDAL helpfully reprojects and converts the filetype to TIF
	# This way we don't have to use the GRIB extension for GeoServer
	# The NAM GRIB projection is weird, so let's just convert to 4326
	echo "Reprojecting and converting, saving as $TIFF."
	gdalwarp $OUTPUTFILE $TIFF -t_srs EPSG:4326 -overwrite -multi -wo SOURCE_EXTRA=1000 --config CENTER_LONG 0 -co TILED=YES -co COMPRESS=NONE

	# !!! Your GeoServer username and password goes here	
	#echo "PUT -> GeoServer"
	#curl -u "wx4web:" -XPUT -H "Content-type: image/tiff" --data-binary @$TIFF -sS  http://localhost:8080/geoserver/rest/workspaces/namhres/coveragestores/$7$5/file.geotiff > /dev/null

	# Delete the GRIB
	# (do we need to delete the TIF too? I forget
	# if GeoServer moves it to its internal storage)
	rm $OUTPUTFILE
	echo "... Done."
	echo "-----------------------------"
	echo ""
}

# Loops through the 0-48 HIRESW forecast and grabs the file
# for each specified parameter type (temp, wind, precip, etc.)
function download_files {
	echo "Downloading files."
	for i in `seq -f %02g 00 48`;
        do
		
		FILENAME="hiresw.t"
		FILENAME+=$1
		FILENAME+=".arw_5km.f"
		FILENAME+=$i
		FILENAME+=".conus.grib2"
		download_and_convert "var_TMP=on" "lev_surface=on" $2 $FILENAME $i "Surface Temp" "SurfaceTemp" "/wxdata/hiresw/tmp/" $1
		download_and_convert "var_APCP=on" "lev_surface=on" $2 $FILENAME $i "Precip Accum" "PrecipAccum" "/wxdata/hiresw/apcp/" $1
		download_and_convert "var_CAPE=on" "lev_surface=on" $2 $FILENAME $i "CAPE" "CAPE" "/wxdata/hiresw/cape/" $1
		download_and_convert "var_CIN=on" "lev_surface=on" $2 $FILENAME $i "CIN" "CIN" "/wxdata/hiresw/cin/" $1
		download_and_convert "var_GUST=on" "lev_surface=on" $2 $FILENAME $i "Gust Speed" "GustSpeed" "/wxdata/hiresw/gust/" $1
		download_and_convert "var_TCDC=on" "lev_entire_atmosphere_%5C%28considered_as_a_single_layer%5C%29=on" $2 $FILENAME $i "Cloud Cover" "CloudCover" "/wxdata/hiresw/tcdc/" $1
		download_and_convert "var_4LFTX=on" "lev_180-0_mb_above_ground=on" $2 $FILENAME $i "Lifted Index" "LiftedIndex" "/wxdata/hiresw/4lftx/" $1
		download_and_convert "var_UGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind U Component" "SurfaceWindU" "/wxdata/hiresw/ugrd/" $1
		download_and_convert "var_VGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind V Component" "SurfaceWindV" "/wxdata/hiresw/vgrd/" $1
		download_and_convert "var_CSNOW=on" "lev_surface=on" $2 $FILENAME $i "Categorical Snow" "CatSnow" "/wxdata/hiresw/csnow/" $1
		download_and_convert "var_CICEP=on" "lev_surface=on" $2 $FILENAME $i "Categorical Ice Pellets" "CatIcePellets" "/wxdata/hiresw/cicep/" $1
		download_and_convert "var_CFRZR=on" "lev_surface=on" $2 $FILENAME $i "Categorical Freezing Rain" "CatFrzRain" "/wxdata/hiresw/cfrzr/" $1
		download_and_convert "var_CRAIN=on" "lev_surface=on" $2 $FILENAME $i "Categorical Rain" "CatRain" "/wxdata/hiresw/crain/" $1
	done
}

# Checks to see if there's new data that needs to be downloaded.
# Keeps track of the last date of the model run with a local file
# 'lastupdated.txt' which just stores the date and time of the model run.
# If the script runs successfully it overwrites this date and time with the latest.
function update_data {
	echo "---------------------"
	echo "Update Data function."
	echo "CHECKDATE: $1"
	echo "CHECKTIME: $2"
	# check if we've already updated this
	read -r lastupdated<"hireswLastUpdated.txt"
	echo "Last updated: $lastupdated"	

	if [ $lastupdated = $1$2 ]; then
		echo "Nevermind, we've got this data already."
		exit 0
	fi
	
	echo "We need to get the new data."
	download_files $2 $1

	echo "Deleting old files..."

	find /wxdata/ -type f -mtime +1 -name '*.tif' -execdir rm -- '{}' \;

	# Exit script
	echo "$1$2" > hireswLastUpdated.txt
	echo "You're all clear kid, now let's blow this thing and go home."
	exit 0
}	

##### MAIN #####
#set -e

echo "------------------"
echo "WX4Web Data Loader"
echo "------------------"
DATE=`date +%Y%m%d`
YEAR=`date +%Y`
MONTH=`date +%m`
DAY=`date +%d`
echo "Initialized date variables."
echo "YEAR: $YEAR"
echo "MONTH: $MONTH"
echo "DAY: $DAY"
echo "------------"

# I can easily imagine a much better way of doing the below
# repetitive tasks...  but whatever.  Since there is considerable
# lag time publishing the models it looks back in time quite a ways.
# Not elegant, but it works.
#
# I didn't really think too much about what time the server actually
# thinks it is.  At least on my test server it uses UTC time
# so the month/day matches that of the model runs.  However
# if your date function is returning local time for some reason
# you may need to look ahead a day to capture the actual UTC date.

CHECKDATE=`date +%Y%m%d`
CHECKTIME="18"
echo "Checking 18Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="12"
echo "Checking 12Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="06"
echo "Checking 06Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="00"
echo "Checking 00Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="18"
echo "Checking 18Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="12"
echo "Checking 12Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="06"
echo "Checking 06Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="00"
echo "Checking 00Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME