#!/bin/bash

# Another script in the GRIBLoader family.
# This one retrieves the SREF data.
#
# Thomas Horner | thomas@14ersForecast.net

# Looks through the remote directory of GRIB files
# to see what the last completed model run was.
function check_if_data_exists {
	FILE="http://nomads.ncep.noaa.gov/pub/data/nccf/com/sref/prod/sref.$1/$2/pgrb/sref_arw.t$2z.pgrb132.ctl.f87.grib2"
	echo "Endpoint: $FILE"

	if curl --output /dev/null --silent --head --fail "$FILE"; then
		echo "------ It exists. ------"
		update_data $1 $2
	else
		echo "It doesn't exist."
	fi
}

# Sets the correct filename to be downloaded, then retrieves the
# GRIB subet from NCEP NOMADS, converts and reprojects it to TIF
function download_and_convert {
	# $1 = model var (TMP, etc.); $2 = model level; $3 = date; $4 = filename; $5 = forecast hour; $6 = label; $7 = output filename; $8 = output directory
	echo ""
	echo "-----------------------------"
	echo "Downloading and converting..."
	echo "Parameter: $6 || Hour: $5"
	
	# Set some filenames
	URL="http://nomads.ncep.noaa.gov/cgi-bin/filter_sref_132.pl?file="
	URL+=$4
	URL+="&"
	URL+=$2
	URL+="&"
	URL+=$1
	URL+="&subregion=&leftlon=250&rightlon=258&toplat=42&bottomlat=36&dir=%2Fsref."
	URL+=$3
    URL+="%2F$9%2Fpgrb"

	SHORTNAME=$8
	SHORTNAME+=$7
	SHORTNAME+=$5

	OUTPUTFILE=$SHORTNAME
	OUTPUTFILE+=".grib2"

	TIFF=$8
	TIFF+=$7
	TIFF+="_"
	TIFF+=`date -d "$3 $9z +$5 hour" +%Y%m%dT%H` 
	TIFF+="0000000Z.tif"
	
	# Download the GRIB subset from NCEP NOMADS
	echo "Downloading as $OUTPUTFILE."
	wget $URL -O $OUTPUTFILE -nv
	
	# GDAL helpfully reprojects and converts the filetype to TIF
	# We're going to use the EPSG:4326 projection
	echo "Reprojecting and converting, saving as $TIFF."
	gdalwarp $OUTPUTFILE $TIFF -t_srs EPSG:4326 -overwrite -multi -wo SOURCE_EXTRA=1000 --config CENTER_LONG 0 -co TILED=YES -co COMPRESS=NONE

	# Delete the GRIB
	rm $OUTPUTFILE
	echo "... Done."
	echo "-----------------------------"
	echo ""
}

# Loops through the 0-36 HREF forecast and grabs the file
# for each specified parameter type (temp, wind, precip, etc.)
function download_files {
	echo "Downloading files."
	for i in `seq -f %02g 00 03 87`;
        do
		
		FILENAME="sref_arw.t"
		FILENAME+=$1
		FILENAME+="z.pgrb132.ctl.f"
		FILENAME+=$i
		FILENAME+=".grib2"
		download_and_convert "var_TMP=on" "lev_surface=on" $2 $FILENAME $i "Surface Temp" "SurfaceTemp" "/wxdata/sref/tmp/" $1
		download_and_convert "var_APCP=on" "lev_surface=on" $2 $FILENAME $i "Precip Accum" "PrecipAccum" "/wxdata/sref/apcp/" $1
		download_and_convert "var_CAPE=on" "lev_surface=on" $2 $FILENAME $i "CAPE" "CAPE" "/wxdata/sref/cape/" $1
		download_and_convert "var_CIN=on" "lev_surface=on" $2 $FILENAME $i "CIN" "CIN" "/wxdata/sref/cin/" $1
		download_and_convert "var_TCDC=on" "lev_entire_atmosphere_%5C%28considered_as_a_single_layer%5C%29=on" $2 $FILENAME $i "Cloud Cover" "CloudCover" "/wxdata/sref/tcdc/" $1
		#download_and_convert "var_SNOD=on" "all_lev=on" $2 $FILENAME $i "Snow Depth" "SnowDepth" "/wxdata/sref/snod/" $1
		# ^ broken.  Please contact NOMADS and tell them to replace SNOWD with SNOD
		download_and_convert "var_PRATE=on" "lev_surface=on" $2 $FILENAME $i "Precipitation Rate" "PrecipRate" "/wxdata/sref/prate/" $1
		download_and_convert "var_4LFTX=on" "lev_180-0_mb_above_ground=on" $2 $FILENAME $i "Lifted Index" "LiftedIndex" "/wxdata/sref/4lftx/" $1
		download_and_convert "var_UGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind U Component" "SurfaceWindU" "/wxdata/sref/ugrd/" $1
		download_and_convert "var_VGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind V Component" "SurfaceWindV" "/wxdata/sref/vgrd/" $1
		download_and_convert "var_CSNOW=on" "lev_surface=on" $2 $FILENAME $i "Categorical Snow" "CatSnow" "/wxdata/sref/csnow/" $1
		download_and_convert "var_CICEP=on" "lev_surface=on" $2 $FILENAME $i "Categorical Ice Pellets" "CatIcePellets" "/wxdata/sref/cicep/" $1
		download_and_convert "var_CFRZR=on" "lev_surface=on" $2 $FILENAME $i "Categorical Freezing Rain" "CatFrzRain" "/wxdata/sref/cfrzr/" $1
		download_and_convert "var_CRAIN=on" "lev_surface=on" $2 $FILENAME $i "Categorical Rain" "CatRain" "/wxdata/sref/crain/" $1
		download_and_convert "var_VIS=on" "lev_surface=on" $2 $FILENAME $i "Visibility" "Visibility" "/wxdata/sref/vis/" $1
		download_and_convert "var_PWAT=on" "lev_entire_atmosphere_%5C%28considered_as_a_single_layer%5C%29=on" $2 $FILENAME $i "PWAT" "PWAT" "/wxdata/sref/pwat/" $1
		download_and_convert "var_RH=on" "lev_2_m_above_ground=on" $2 $FILENAME $i "Relative Humidity" "Humidity" "/wxdata/sref/rh/" $1
	done
}

# Checks to see if there's new data that needs to be downloaded.
# Keeps track of the last date of the model run with a local file
# 'srefLastUpdated.txt' which just stores the date and time of the model run.
# If the script runs successfully it overwrites this date and time with the latest.
function update_data {
	echo "---------------------"
	echo "Update Data function."
	echo "CHECKDATE: $1"
	echo "CHECKTIME: $2z"
	# check if we've already updated this
	read -r lastupdated<"srefLastUpdated.txt"
	echo "Last updated: $lastupdated"	

	if [ $lastupdated = $1$2 ]; then
		echo "Nevermind, we've got this data already."
		exit 0
	fi
	
	echo "We need to get the new data."
	download_files $2 $1

	echo "Deleting old files..."

	find /wxdata/ -type f -mtime +1 -name '*.tif' -execdir rm -- '{}' \;
	find /wxdata/ -type f -mtime +1 -name '*.grib2' -execdir rm -- '{}' \;

	# Exit script
	echo "$1$2" > srefLastUpdated.txt
	echo "You're all clear kid, now let's blow this thing and go home."
	exit 0
}	

##### MAIN #####
#set -e

echo "------------------"
echo "WX4Web Data Loader"
echo "------------------"
DATE=`date +%Y%m%d`
YEAR=`date +%Y`
MONTH=`date +%m`
DAY=`date +%d`
echo "Initialized date variables."
echo "YEAR: $YEAR"
echo "MONTH: $MONTH"
echo "DAY: $DAY"
echo "------------"

# I can easily imagine a much better way of doing the below
# repetitive tasks...  but whatever.  Since there is considerable
# lag time publishing the models it looks back in time quite a ways.
# Not elegant, but it works.
#
# I didn't really think too much about what time the server actually
# thinks it is.  At least on my test server it uses UTC time
# so the month/day matches that of the model runs.  However
# if your date function is returning local time for some reason
# you may need to look ahead a day to capture the actual UTC date.

CHECKDATE=`date +%Y%m%d`
CHECKTIME="21"
echo "Checking 21Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="15"
echo "Checking 15Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="09"
echo "Checking 09Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="03"
echo "Checking 03Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="21"
echo "Checking 21Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="15"
echo "Checking 15Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="09"
echo "Checking 09Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="03"
echo "Checking 03Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME