#!/bin/bash

# Another script in the GRIBLoader family.
# This one retrieves the NAM 12km data.
#
# Thomas Horner | thomas@14ersForecast.net

# Looks through the remote directory of GRIB files
# to see what the last completed model run was.
function check_if_data_exists {
	FILE="http://nomads.ncep.noaa.gov/pub/data/nccf/com/nam/prod/nam.$1/nam.t$2.awphys84.tm00.grib2"
	echo "Endpoint: $FILE"

	if curl --output /dev/null --silent --head --fail "$FILE"; then
		echo "------ It exists. ------"
		update_data $1 $2
	else
		echo "It doesn't exist."
	fi
}

# Sets the correct filename to be downloaded, then retrieves the
# GRIB subet from NCEP NOMADS, converts and reprojects it to TIF.
function download_and_convert {
	# $1 = model var (TMP, etc.); $2 = model level; $3 = date; $4 = filename; $5 = forecast hour; $6 = label; $7 = output filename; $8 = output directory
	echo ""
	echo "-----------------------------"
	echo "Downloading and converting..."
	echo "Parameter: $6 || Hour: $5"
	
	# Set some filenames
	URL="http://nomads.ncep.noaa.gov/cgi-bin/filter_nam.pl?file="
	URL+=$4
	URL+="&"
	URL+=$2
	URL+="&"
	URL+=$1
	URL+="&subregion=&leftlon=250&rightlon=258&toplat=42&bottomlat=36&dir=%2Fnam."
	URL+=$3

	SHORTNAME=$8
	SHORTNAME+=$7
	SHORTNAME+=$5

	OUTPUTFILE=$SHORTNAME
	OUTPUTFILE+=".grib2"

	TIFF=$8
	TIFF+=$7
	TIFF+="_"
	TIFF+=`date -d "$3 $9 +$5 hour" +%Y%m%dT%H` 
	TIFF+="0000000Z.tif"
	
	# Download the GRIB subset from NCEP NOMADS
	echo "Downloading as $OUTPUTFILE."
	wget $URL -O $OUTPUTFILE -nv
	
	# GDAL helpfully reprojects and converts the filetype to TIF
	# We're going to use the EPSG:4326 projection
	echo "Reprojecting and converting, saving as $TIFF."
	gdalwarp $OUTPUTFILE $TIFF -t_srs EPSG:4326 -overwrite -multi -wo SOURCE_EXTRA=1000 --config CENTER_LONG 0 -co TILED=YES -co COMPRESS=NONE

	# Delete the GRIB
	rm $OUTPUTFILE
	echo "... Done."
	echo "-----------------------------"
	echo ""
}

# Loops through the 0-84 hour NAM forecast and grabs the file
# for each specified parameter type (temp, wind, precip, etc.)
function download_files {
	echo "Downloading files."
	for i in `seq -f %02g 00 84`;
        do
		
		FILENAME="nam.t"
		FILENAME+=$1
		FILENAME+=".awphys"
		FILENAME+=$i
		FILENAME+=".tm00.grib2"
		download_and_convert "var_TMP=on" "lev_surface=on" $2 $FILENAME $i "Surface Temp" "SurfaceTemp" "/wxdata/nam/tmp/" $1
		download_and_convert "var_SNOD=on" "lev_surface=on" $2 $FILENAME $i "Snow Depth" "SnowDepth" "/wxdata/nam/snod/" $1
		#download_and_convert "var_PRATE=on" "lev_surface=on" $2 $FILENAME $i "Precipitation Rate" "PrecipRate" "/wxdata/nam/prate/" $1
		download_and_convert "var_APCP=on" "lev_surface=on" $2 $FILENAME $i "Precip Accum" "PrecipAccum" "/wxdata/nam/apcp/" $1
		download_and_convert "var_LTNG=on" "lev_surface=on" $2 $FILENAME $i "Lightning" "Lightning" "/wxdata/nam/ltng/" $1
		download_and_convert "var_CAPE=on" "lev_surface=on" $2 $FILENAME $i "CAPE" "CAPE" "/wxdata/nam/cape/" $1
		download_and_convert "var_CIN=on" "lev_surface=on" $2 $FILENAME $i "CIN" "CIN" "/wxdata/nam/cin/" $1
		download_and_convert "var_GUST=on" "lev_surface=on" $2 $FILENAME $i "Gust Speed" "GustSpeed" "/wxdata/nam/gust/" $1
		download_and_convert "var_TCDC=on" "lev_entire_atmosphere_%5C%28considered_as_a_single_layer%5C%29=on" $2 $FILENAME $i "Cloud Cover" "CloudCover" "/wxdata/nam/tcdc/" $1
		download_and_convert "var_4LFTX=on" "lev_180-0_mb_above_ground=on" $2 $FILENAME $i "Lifted Index" "LiftedIndex" "/wxdata/nam/4lftx/" $1
		download_and_convert "var_UGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind U Component" "SurfaceWindU" "/wxdata/nam/ugrd/" $1
		download_and_convert "var_VGRD=on" "lev_10_m_above_ground=on" $2 $FILENAME $i "Wind V Component" "SurfaceWindV" "/wxdata/nam/vgrd/" $1
		download_and_convert "var_CSNOW=on" "lev_surface=on" $2 $FILENAME $i "Categorical Snow" "CatSnow" "/wxdata/nam/csnow/" $1
		download_and_convert "var_CICEP=on" "lev_surface=on" $2 $FILENAME $i "Categorical Ice Pellets" "CatIcePellets" "/wxdata/nam/cicep/" $1
		download_and_convert "var_CFRZR=on" "lev_surface=on" $2 $FILENAME $i "Categorical Freezing Rain" "CatFrzRain" "/wxdata/nam/cfrzr/" $1
		download_and_convert "var_CRAIN=on" "lev_surface=on" $2 $FILENAME $i "Categorical Rain" "CatRain" "/wxdata/nam/crain/" $1
	done
}

# Checks to see if there's new data that needs to be downloaded.
# Keeps track of the last date of the model run with a local file
# 'namLastUpdated.txt' which just stores the date and time of the model run.
# If the script runs successfully it overwrites this date and time with the latest.
function update_data {
	echo "---------------------"
	echo "Update Data function."
	echo "CHECKDATE: $1"
	echo "CHECKTIME: $2"
	# check if we've already updated this
	read -r lastupdated<"namLastUpdated.txt"
	echo "Last updated: $lastupdated"	

	if [ $lastupdated = $1$2 ]; then
		echo "Nevermind, we've got this data already."
		exit 0
	fi
	
	echo "We need to get the new data."
	download_files $2 $1

	echo "Done, now cleaning up."
	echo "Deleting old files..."

	find /wxdata/ -type f -mtime +1 -name '*.tif' -execdir rm -- '{}' \;

	# Exit script
	echo "$1$2" > namLastUpdated.txt
	echo "You're all clear kid, now let's blow this thing and go home."
	exit 0
}	

##### MAIN #####
#set -e

echo "------------------"
echo "WX4Web Data Loader"
echo "------------------"
DATE=`date +%Y%m%d`
YEAR=`date +%Y`
MONTH=`date +%m`
DAY=`date +%d`
echo "Initialized date variables."
echo "YEAR: $YEAR"
echo "MONTH: $MONTH"
echo "DAY: $DAY"
echo "------------"

# I can easily imagine a much better way of doing the below
# repetitive tasks...  but whatever.  Since there is considerable
# lag time publishing the models it looks back in time quite a ways.
# Not elegant, but it works.
#
# I didn't really think too much about what time the server actually
# thinks it is.  At least on my test server it uses UTC time
# so the month/day matches that of the model runs.  However
# if your date function is returning local time for some reason
# you may need to look ahead a day to capture the actual UTC date.

CHECKDATE=`date +%Y%m%d`
CHECKTIME="18z"
echo "Checking 18Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="12z"
echo "Checking 12Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="06z"
echo "Checking 06Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date +%Y%m%d`
CHECKTIME="00z"
echo "Checking 00Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="18z"
echo "Checking 18Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="12z"
echo "Checking 12Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="06z"
echo "Checking 06Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME

CHECKDATE=`date -d '-1 day' +%Y%m%d`
CHECKTIME="00z"
echo "Checking 00Z $CHECKDATE"
check_if_data_exists $CHECKDATE $CHECKTIME
